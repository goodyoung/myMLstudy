{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c46442d-125e-4673-b087-29f3482bcc4a",
   "metadata": {},
   "source": [
    "# 전이 학습\n",
    "아주 큰 데이터셋을 써서 훈련된 모델의 가중치를 가져와 우리가 해결하려는 과제에 맞게 보정해서 사용하는 것.\n",
    "\n",
    "전이 학습을 위한 방법\n",
    "- 특성 추출 (feature extractor)\n",
    "- 미세 조정 기법 (fine tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97a67a-ce27-4a0c-8a40-6c258190f9b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 특성 추출 (feture extractor)\n",
    "\n",
    "사전 훈련된 모델을 가져온 후 FC 부분만 새로 만든다.\n",
    "학습 시 FC부분만 학습하고 나머지 계층들은 학습 되지 않도록 한다.\n",
    "\n",
    "**특성 추출은 목표 특성을 잘 추출했다는 전제하에 좋은 성능을 낼 수 있다. 그게 아니면 미세 조정 기법으로 대체**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfebaf3-80e8-483a-8123-4c4a5d298745",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "116def6a-db89-4e25-b397-060d00e21f90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/goodyoung/Desktop/GIt/myMLstudy/.mlvenv/lib/python3.11/site-packages (from opencv-python) (1.26.0)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl (55.7 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84749071-0e68-477e-a58e-91ac68465a18",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/goodyoung/Desktop/GIt/myMLstudy/.mlvenv/lib/python3.11/site-packages/cv2/cv2.abi3.so, 2): Symbol not found: __ZNSt3__113basic_filebufIcNS_11char_traitsIcEEE4openEPKcj\n  Referenced from: /Users/goodyoung/Desktop/GIt/myMLstudy/.mlvenv/lib/python3.11/site-packages/cv2/.dylibs/libvmaf.1.dylib (which was built for Mac OS X 12.0)\n  Expected in: /usr/lib/libc++.1.dylib\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/goodyoung/Desktop/GIt/myMLstudy/.mlvenv/lib/python3.11/site-packages/cv2/cv2.abi3.so, 2): Symbol not found: __ZNSt3__113basic_filebufIcNS_11char_traitsIcEEE4openEPKcj\n  Referenced from: /Users/goodyoung/Desktop/GIt/myMLstudy/.mlvenv/lib/python3.11/site-packages/cv2/.dylibs/libvmaf.1.dylib (which was built for Mac OS X 12.0)\n  Expected in: /usr/lib/libc++.1.dylib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3d435-5707-4413-bf0a-d6bd72b23972",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6980e0f-4fdd-4681-813a-e7a33cc6a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/catanddog/train'\n",
    "    \n",
    "transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize([256, 256]), #이미지의 크기를 256 by 256으로 조정\n",
    "                    transforms.RandomResizedCrop(size=(224,224), scale = (0.1,1), ratio=(0.5,2)), # 이미지를 랜덤한 크기 및 비율로 자른다. (데이터 확장 용도.)\n",
    "                    #면적 비율을 0.1~1 범위 내에서 무작위로 자른다(scale), 면적의 너비와 높이 비율을 0.5~2 범위 내에서 무작위로 조절한다(ratio).\n",
    "                    transforms.RandomHorizontalFlip(), # 랜덤하게 수평으로 뒤집는다.\n",
    "                    transforms.ToTensor(), # 이미지 데이터를 텐서로 변환\n",
    "                ])\n",
    "train_dataset = torchvision.datasets.ImageFolder( #경로 및 전처리를 함께 포함하여 불러온다.\n",
    "    data_path,\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e77b8-3a2f-4548-afc5-aff3e756f965",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 레이블 정보와 함께 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb28e35-bd62-4119-a5ef-940a00e8f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = iter(train_loader).next() #iter()는 전달된 데이터의 반복자를 꺼내 반환하며 next()는 다음에 출력해야 할 요소를 반환한다. train_loader에서 데이터를 하나씩 꺼내옴.\n",
    "classes = {0:'cat', 1:'dog'}\n",
    "fig = plt.figure(figsize=(16,24))\n",
    "for i in range(24):\n",
    "    a = fig.add_subplot(4,6,i+1)\n",
    "    a.set_title(classes[labels[i].item()])\n",
    "    a.axis('off')\n",
    "    a.imshow(np.transpose(samples[i].numpy(), (1,2,0)))\n",
    "plt.subplots_adjust(bottom=0.2, top=0.6, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdcdcc2-d780-44ea-9a2b-0f3fa0459d72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33def7e1-3208-4767-8a5e-807ff32ba621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c277efe-9ebd-4ac4-a8b9-a4af365ad00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goodyoung/Desktop/GIt/myMLstudy/.mlvenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/goodyoung/Desktop/GIt/myMLstudy/.mlvenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# resnet18 = models.resnet18() #무작위의 가중치\n",
    "resnet18 = models.resnet18(pretrained=True) #사전 학습된 모델의 가중치 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f47b6fd2-f96f-4110-b767-a0510b02900f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": False\n",
      ": True\n",
      ": True\n"
     ]
    }
   ],
   "source": [
    "for i in resnet18.parameters():\n",
    "    print(f\": {i.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71577875-7422-4aa8-b23d-faa5a4afc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성곱층 사용하지만 그 해당 구간에 대해 학습을 하지 않도록 고정시킴.\n",
    "def set_parameter_requires_grad(model, feature_extracting=True):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "set_parameter_requires_grad(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbcbda2e-30dd-4e0c-9fe5-e68b1c50c932",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5035de3b-e5de-4ad3-b684-dda4230b6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc = nn.Linear(512, 2) # fc의 out_features가 1000에서 2로 줄어들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88a1bf1a-c445-42b3-9405-e94b648e4dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.weight tensor([[-0.0380,  0.0094,  0.0075,  ...,  0.0203, -0.0333,  0.0119],\n",
      "        [ 0.0224, -0.0351,  0.0090,  ..., -0.0219,  0.0241,  0.0183]])\n",
      "fc.bias tensor([-0.0206, -0.0178])\n"
     ]
    }
   ],
   "source": [
    "# 이 두개의 파라미터를 변환하면 된다.\n",
    "for name, param in resnet18.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f9512-ddb9-4a6e-9567-8f97afdab438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파 중 파라미터들에 대한 변화를 계산할 필요가 없음을 나타낸다.\n",
    "# transfer learning main code\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = torch.nn.Linear(512, 2)\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters()) # 여기서 fc 파라미터들만 준다.\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53ab12-9a07-4d50-a116-caf0c0222062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=13, is_train=True):\n",
    "    since = time.time()    \n",
    "    acc_history = []\n",
    "    loss_history = []\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            model.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1) # 텐서 배열의 최대값이 들어 있는 idx를 반환하는 함수이다.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "\n",
    "        acc_history.append(epoch_acc.item())\n",
    "        loss_history.append(epoch_loss)        \n",
    "        torch.save(model.state_dict(), os.path.join('../chap05/data/catanddog/', '{0:0=2d}.pth'.format(epoch)))\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Acc: {:4f}'.format(best_acc))    \n",
    "    return acc_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4174f665-2e25-4463-883b-37dff7789bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t fc.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0380,  0.0094,  0.0075,  ...,  0.0203, -0.0333,  0.0119],\n",
      "        [ 0.0224, -0.0351,  0.0090,  ..., -0.0219,  0.0241,  0.0183]],\n",
      "       requires_grad=True)\n",
      "\t fc.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0206, -0.0178], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FC층은 학습을 하도록 학습을 통해 얻어지는 파라미터를 옵티마이저에 전달해서 최종적으로 모델 학습에 사용합니다.\n",
    "# 학습을 하면 그럼 이 파라미터들이 업데이트 될 것 이다.\n",
    "params_to_update = []\n",
    "for name,param in resnet18.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "            \n",
    "optimizer = optim.Adam(params_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639127a-f549-46e1-8081-527e233206e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_acc_hist, train_loss_hist = train_model(resnet18, train_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164c6e1-e256-4206-9f80-b8ce708ec430",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343af73-53cc-464c-a77b-fd80ec6c1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './data/catanddog/train'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                ])\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=test_path,\n",
    "    transform=transform\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01efd75-22b0-48a5-9a58-54a6a3e15680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloaders, device):\n",
    "    since = time.time()    \n",
    "    acc_history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    saved_models = glob.glob('../chap05/data/catanddog/' + '*.pth')\n",
    "    saved_models.sort()\n",
    "    print('saved_model', saved_models)\n",
    "\n",
    "    for model_path in saved_models:\n",
    "        print('Loading model', model_path)\n",
    "\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)           \n",
    "            preds[preds >= 0.5] = 1\n",
    "            preds[preds < 0.5] = 0\n",
    "            running_corrects += preds.eq(labels.cpu()).int().sum() # preds 배열과 labels가 일치하는지 검사하는 용도\n",
    "            #.sum은 예측 결과와 정답이 일치하는 것들의 개수를 합.\n",
    "            \n",
    "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "        print('Acc: {:.4f}'.format(epoch_acc))\n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "\n",
    "        acc_history.append(epoch_acc.item())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    return acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087fd309-5cd2-45f6-8009-54957d1e194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_hist = eval_model(resnet18, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7b309-2db9-43a2-8a77-7da6620c5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):  \n",
    "    image=tensor.clone().detach().numpy()   # 메모리에는 새롭게 올리지만 기울기는 전파하지 않겠다.\n",
    "    image=image.transpose(1,2,0)  \n",
    "    image=image*(np.array((0.5,0.5,0.5))+np.array((0.5,0.5,0.5)))  \n",
    "    image=image.clip(0,1)   #0과 1 사이로 제한\n",
    "    return image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a5902-b7ba-4bf7-9733-a9cea5880b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞는지 아닌지 출력\n",
    "classes = {0:'cat', 1:'dog'}\n",
    "\n",
    "dataiter=iter(test_loader)  \n",
    "images,labels=dataiter.next()  \n",
    "output=model(images)  \n",
    "_,preds=torch.max(output,1) \n",
    "\n",
    "fig=plt.figure(figsize=(25,4))  \n",
    "for idx in np.arange(20):  \n",
    "    ax=fig.add_subplot(2,10,idx+1,xticks=[],yticks=[])  \n",
    "    plt.imshow(im_convert(images[idx]))  \n",
    "    a.set_title(classes[labels[i].item()])\n",
    "    ax.set_title(\"{}({})\".format(str(classes[preds[idx].item()]),str(classes[labels[idx].item()])),color=(\"green\" if preds[idx]==labels[idx] else \"red\"))  \n",
    "plt.show()  \n",
    "plt.subplots_adjust(bottom=0.2, top=0.6, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26360fa9-012f-40ff-904d-b668472646f4",
   "metadata": {},
   "source": [
    "# 미세 조정 기법 (fine-tuning)\n",
    "\n",
    "특성 추출 기법에서 더 나아가 합성곱층, fc의 가중치를 업데이트하여 훈련시키는 방식입니다.\n",
    "\n",
    "**즉 사전 학습된 모델을 목적에 맞게 재학습시키거나 학습된 가중치의 일부를 재학습시키는 것이다.**\n",
    "\n",
    "## **미세 조정 전략**\n",
    "- 데이터셋 크고, 모델과 유사성이 작은 경우\n",
    "  - 모델 전체 ***재학습***\n",
    "- 데이터셋 크고, 모델과 유사성이 큰 경우\n",
    "  - 합성곱층의 뒷부분 (FC와 가까운 부분)과 데이터 분류기를 학습시킴. 데이터 셋 유사성이 크기 때문이다.\n",
    "- 데이터셋 작고, 모델과 유사성이 작은 경우\n",
    "  - 합성곱층의 일부분과 데이터 분류기(FC)를 학습시킨다. 데이터가 적어서 효과가 없을 수 있다. 적당히 설정해야한다.\n",
    "- 데이터셋 작고, 모델과 유사성이 큰 경우\n",
    "  - 데이터 분류기만 학습시킨다. 데이터가 적기 때문에 많은 계층에 적용하면 과적합이 발생할 수 있어서 FC에 대해서만 적용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
