{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00108550-0fae-4996-a17a-99aa2f6d649d",
   "metadata": {},
   "source": [
    "## DDPM 기초 구현\n",
    "\n",
    "### 구현해야될 항목\n",
    "  - 각종 변수들 ($\\alpha, \\tilde\\alpha, \\mu, ...$)\n",
    "  - 훈련 코드\n",
    "  - 샘플링 코드\n",
    "\n",
    "## 참조\n",
    "참고한 코드 출처: https://github.com/CodingVillainKor/SimpleDeepLearning/blob/main/DDPM_notebook.ipynb \n",
    "\n",
    "위 레포지토리의 원본 구현을 참고하여 수정하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eef6e-dbfd-4cd4-98d5-3c6a2168079a",
   "metadata": {},
   "source": [
    "### 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a74eb4-1a30-4d8c-80e2-b1972bd32be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# check cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    torch.device(\"cpu\")\n",
    "print(\" Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba63e6-ad32-4be5-b64c-fcbc44c31c2a",
   "metadata": {},
   "source": [
    "### 기본 계수 코드\n",
    "<details>\n",
    "  <summary> torch 예제 코드 (cumprod, pad 함수) </summary>\n",
    "    \n",
    "1. cumprod 함수 예제\n",
    "    \n",
    "    ``` python\n",
    "    \n",
    "    a = torch.tensor([5, 7, 10])\n",
    "    torch.cumprod(a, dim = 0)\n",
    "    # 출력: tensor([  5,  35, 350])\n",
    "    \n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "2. pad 함수 예제\n",
    "\n",
    "    ```python\n",
    "    a = torch.tensor([1,2,3,4])\n",
    "    \n",
    "    # 앞쪽에 2개, 뒤쪽에 3개 패딩 추가 , 기본 값 0 -> 9\n",
    "    F.pad(a, (2, 3), value = 9)\n",
    "    # 출력:: tensor([9, 9, 1, 2, 3, 4, 9, 9, 9])\n",
    "    ```\n",
    "</details>\n",
    "\n",
    "- 기본 계수들은 $T$ 크기만큼 계산이 완료가 되어있는 tensor의 상태로 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25abe153-f1e1-4474-ba64-7487f269ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time t\n",
    "T = 1000 \n",
    "\n",
    "# beta: linear하게 증가\n",
    "betas = torch.linspace(1e-4, 0.02, T).to(device) \n",
    "\n",
    "# alpha는 beta의 변형이므로\n",
    "alphas = 1 - betas\n",
    "\n",
    "# alpha bar는 alhpa의 누적 합 -> torch의 cumprod 함수\n",
    "alphas_bar = torch.cumprod(alphas, dim = 0, ).to(device)\n",
    "\n",
    "# alpha bar 의 t-1도 sampling 과정에서 필요하다. \n",
    "# 맨 처음에 alpha가 하나도 없었다는 뜻으로 맨 앞에 1을 추가.\n",
    "alphas_bar_prev = F.pad(alphas_bar[:-1], (1, 0), value = 1)\n",
    "\n",
    "#training에 필요한 변수\n",
    "sqrt_alphas_bar = torch.sqrt(alphas_bar).to(device)\n",
    "sqrt_one_minus_alphas_bar = torch.sqrt(1. - alphas_bar).to(device)\n",
    "\n",
    "# sampling에 필요한 변수\n",
    "reciprocal_alphas_sqrt = torch.sqrt(1. / alphas_bar).to(device)\n",
    "reciprocal_alphasm1_sqrt = torch.sqrt(1. / alphas_bar - 1.).to(device)\n",
    "posterior_mean_coef1 = torch.sqrt(alphas_bar_prev) * betas / (1. - alphas_bar).to(device)\n",
    "posterior_mean_coef2 = torch.sqrt(alphas) * (1. - alphas_bar_prev) / (1. - alphas_bar).to(device)\n",
    "sigmas = (betas * (1. - alphas_bar_prev) / (1. - alphas_bar)).to(device) # 시그마는 betas를 사용해도 무관함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba39202-e5da-4337-a0c2-f608b3a9781f",
   "metadata": {},
   "source": [
    "### 훈련 코드\n",
    "\n",
    "1. **repeat**:\n",
    "   - \\($ \\mathbf{x}_0 \\sim q(\\mathbf{x}_0) $\\)  (데이터 분포에서 샘플링)\n",
    "   - \\($ t \\sim \\text{Uniform}(\\{1, \\dots, T\\})$ \\)  (랜덤한 시간 스텝 샘플링)\n",
    "   - \\($ \\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ \\)  (정규분포에서 노이즈 샘플링)\n",
    "   - Gradient descent step on:\n",
    "\n",
    "   - $\n",
    "     \\nabla_\\theta \\left\\| \\epsilon - \\epsilon_\\theta \\left( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t \\right) \\right\\|^2\n",
    "     $\n",
    "\n",
    "2. **until converged**\n",
    "\n",
    "<details>\n",
    "  <summary> torch 예제 코드 (gather, view 함수) </summary>\n",
    "\n",
    "1. gather 함수\n",
    "    \n",
    "    ```python\n",
    "        # 원본 텐서\n",
    "        coeff = torch.tensor([10, 20, 30, 40, 50])\n",
    "        \n",
    "        # 특정 인덱스를 가져올 t 텐서\n",
    "        t = torch.tensor([0, 2, 4])  # 인덱스 0, 2, 4를 가져오도록 지정\n",
    "        \n",
    "        # gather 사용 (dim=0)\n",
    "        coeff_t = torch.gather(coeff, index=t, dim=0)\n",
    "        # 출력: tensor([10, 30, 50])\n",
    "    ```\n",
    "    \n",
    "2. view 함수\n",
    "\n",
    "    ```python\n",
    "    # 예제 텐서\n",
    "    coeff_t = torch.tensor([1, 2, 3, 4])  # Shape: (4,)\n",
    "    \n",
    "    # 변환할 차원 리스트\n",
    "    dims = [2, 2]  # 여기서 len(dims) = 2\n",
    "    \n",
    "    # 새로운 차원으로 변환\n",
    "    B = coeff_t.shape[0] # 4\n",
    "    reshaped_tensor = coeff_t.view([B] + [1] * len(dims))\n",
    "    # 출력: torch.Size([4, 1, 1])\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c11c7-999c-45fe-ab5b-6d2655d322ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_and_expand(coeff, t, xshape):\n",
    "    ''' \n",
    "    t시간에 해당하는 계수(인덱스)를 계수 텐서(coeff)에서 가져오고, \n",
    "    해당 계수들을 batch size에 맞게 확장하는 함수 \n",
    "    '''\n",
    "    # 입력 텐서의 차원 분리\n",
    "    batch_size, *dims = xshape # batch_size는 첫 번째 차원, dims는 나머지 차원\n",
    "\n",
    "     # t시간에 해당하는 계수 가져오기\n",
    "    coeff_t = torch.gather(coeff, index = t, dim = 0) # Shape: (len(t), ) = (batch_size, )\n",
    "    \n",
    "    # 차원 확장\n",
    "    coeff_t = coeff_t.view([batch_size] + [1] * len(dims)) # 나머지 차원들에 맞게 확장 -> 이후의 계산 차원을 맞추기 위하여\n",
    "    return coeff_t\n",
    "    \n",
    "def train(model, x_0): # x_0: 데이터 분포에서 샘플링한 입력 데이터\n",
    "    # 랜덤한 시간 스텝에서 배치 사이즈(x_0.shape[0])만큼 샘플링\n",
    "    t = torch.randint(T, size = (x_0.shape[0], ), device = x_0.device)\\\n",
    "    \n",
    "    # 정규분포에서 노이즈 샘플링, shape이 batch와 같게\n",
    "    eps = torch.randn_like(x_0)\n",
    "    \n",
    "    # model input, batch 들간 계수\n",
    "    # 모든 batch가 같은 t를 사용하지 않기 때문에 해당 함수를 사용해야함\n",
    "    x_t = gather_and_expand(sqrt_alphas_bar, t, x_0.shape)*x_0 + gather_and_expand(sqrt_one_minus_alphas_bar, t, x_0.shape) * eps\n",
    "    \n",
    "    # eps와 model output mse 구하기\n",
    "    loss = F.mse_loss(model(x_t, t), eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46c59a-e665-478b-b8e1-ddf0801cc9d4",
   "metadata": {},
   "source": [
    "### 샘플링 코드\n",
    "1. **Initialize**:  \n",
    "       - \\($ \\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) $\\)  (시작 샘플은 표준 정규분포에서 샘플링)\n",
    "    \n",
    "2. **For \\( t = T, ..., 1 \\) do**:\n",
    "   - \\($ \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ \\) if \\($ t > 1 $\\), else \\($ \\mathbf{z} = 0 $\\)  \n",
    "     (마지막 단계가 아니면 가우시안 노이즈 추가)\n",
    "\n",
    "   - 업데이트:\n",
    "     $\n",
    "     \\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \n",
    "     \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(\\mathbf{x}_t, t) \\right) \n",
    "     + \\sigma_t \\mathbf{z}\n",
    "     $\n",
    "       - 구현 시엔 $\\mu_\\theta(x_t,t) = \\tilde{\\mu}_t \\left( \\mathbf{x}_t, \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( \\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta} (\\mathbf{x}_t) \\right) \\right)$ 의 수식을 사용하여 구현한다.\n",
    "       - 따라서 다음과 같아서 $ \\tilde{\\mu}_t (\\mathbf{x}_t, \\mathbf{x}_0) := \n",
    "\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0 \n",
    "+ \\frac{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t$ , $x_0$은 다음과 같아진다 $x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \n",
    "\\left( \\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta} (\\mathbf{x}_t) \\right)$\n",
    "\n",
    "3. **Return** \\( $\\mathbf{x}_0 $\\)  (최종 생성된 샘플)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56118c-d904-4648-a82c-31665d48613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#기존\n",
    "def sample(model, x_T): # x_T: noisy data\n",
    "    x_t = x_T\n",
    "    for time_step in reversed(range(T)): # T , ..., 1 반복 수행\n",
    "        # 각 time_step를 batch size로 확장\n",
    "        t = torch.full((x_T.shape[0], ), time_step, dtype=torch.long, device=device)\n",
    "        \n",
    "        # 마지막 단계가 아니면 가우시안 노이즈 샘플링\n",
    "        z = torch.randn_like(x_t) if time_step else 0\n",
    "\n",
    "        # 업데이트 과정\n",
    "        eps = model(x_t, t) # 모델 예측\n",
    "\n",
    "        # x_0을 구함\n",
    "        print(f\"기존 차원: {gather_and_expand(reciprocal_alphas_sqrt, t, eps.shape).shape} \\n  \\\n",
    "                view 차원: {reciprocal_alphas_sqrt[t].view(eps.shape).shape}\")\n",
    "              \n",
    "        x0_predicted = gather_and_expand(reciprocal_alphas_sqrt, t, eps.shape) * x_t - \\\n",
    "            gather_and_expand(reciprocal_alphasm1_sqrt, t, eps.shape) * eps\n",
    "        \n",
    "        # 위 x_0과 함께 평균을 구한다\n",
    "        mean = gather_and_expand(posterior_mean_coef1, t, eps.shape) * x0_predicted + \\\n",
    "            gather_and_expand(posterior_mean_coef2, t, eps.shape) * x_t\n",
    "        \n",
    "        # 분산 구함\n",
    "        var = torch.sqrt(gather_and_expand(sigmas, t, eps.shape)) * z\n",
    "\n",
    "        x_t = mean + var\n",
    "    x_0 = x_t\n",
    "    return x_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
