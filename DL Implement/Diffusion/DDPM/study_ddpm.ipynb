{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00108550-0fae-4996-a17a-99aa2f6d649d",
   "metadata": {},
   "source": [
    "## DDPM ê¸°ì´ˆ êµ¬í˜„\n",
    "\n",
    "### êµ¬í˜„í•´ì•¼ë  í•­ëª©\n",
    "  - ê°ì¢… ë³€ìˆ˜ë“¤ ($\\alpha, \\tilde\\alpha, \\mu, ...$)\n",
    "  - í›ˆë ¨ ì½”ë“œ\n",
    "  - ìƒ˜í”Œë§ ì½”ë“œ\n",
    "\n",
    "## ì°¸ì¡°\n",
    "ì°¸ê³ í•œ ì½”ë“œ ì¶œì²˜: https://github.com/CodingVillainKor/SimpleDeepLearning/blob/main/DDPM_notebook.ipynb \n",
    "\n",
    "ìœ„ ë ˆí¬ì§€í† ë¦¬ì˜ ì›ë³¸ êµ¬í˜„ì„ ì°¸ê³ í•˜ì—¬ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eef6e-dbfd-4cd4-98d5-3c6a2168079a",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a74eb4-1a30-4d8c-80e2-b1972bd32be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# visualization\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    torch.device(\"cpu\")\n",
    "print(\" Device: \", device)\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba63e6-ad32-4be5-b64c-fcbc44c31c2a",
   "metadata": {},
   "source": [
    "### ê¸°ë³¸ ê³„ìˆ˜ ì½”ë“œ\n",
    "<details>\n",
    "  <summary> torch ì˜ˆì œ ì½”ë“œ (cumprod, pad í•¨ìˆ˜) </summary>\n",
    "    \n",
    "1. cumprod í•¨ìˆ˜ ì˜ˆì œ\n",
    "    \n",
    "    ``` python\n",
    "    \n",
    "    a = torch.tensor([5, 7, 10])\n",
    "    torch.cumprod(a, dim = 0)\n",
    "    # ì¶œë ¥: tensor([  5,  35, 350])\n",
    "    \n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "2. pad í•¨ìˆ˜ ì˜ˆì œ\n",
    "\n",
    "    ```python\n",
    "    a = torch.tensor([1,2,3,4])\n",
    "    \n",
    "    # ì•ìª½ì— 2ê°œ, ë’¤ìª½ì— 3ê°œ íŒ¨ë”© ì¶”ê°€ , ê¸°ë³¸ ê°’ 0 -> 9\n",
    "    F.pad(a, (2, 3), value = 9)\n",
    "    # ì¶œë ¥:: tensor([9, 9, 1, 2, 3, 4, 9, 9, 9])\n",
    "    ```\n",
    "</details>\n",
    "\n",
    "- ê¸°ë³¸ ê³„ìˆ˜ë“¤ì€ $T$ í¬ê¸°ë§Œí¼ ê³„ì‚°ì´ ì™„ë£Œê°€ ë˜ì–´ìˆëŠ” tensorì˜ ìƒíƒœë¡œ ìƒê°í•˜ë©´ ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25abe153-f1e1-4474-ba64-7487f269ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time t\n",
    "T = 1000 \n",
    "\n",
    "# beta: linearí•˜ê²Œ ì¦ê°€\n",
    "betas = torch.linspace(1e-4, 0.02, T).to(device) \n",
    "\n",
    "# alphaëŠ” betaì˜ ë³€í˜•ì´ë¯€ë¡œ\n",
    "alphas = 1 - betas\n",
    "\n",
    "# alpha barëŠ” alhpaì˜ ëˆ„ì  í•© -> torchì˜ cumprod í•¨ìˆ˜\n",
    "alphas_bar = torch.cumprod(alphas, dim = 0, ).to(device)\n",
    "\n",
    "# alpha bar ì˜ t-1ë„ sampling ê³¼ì •ì—ì„œ í•„ìš”í•˜ë‹¤. \n",
    "# ë§¨ ì²˜ìŒì— alphaê°€ í•˜ë‚˜ë„ ì—†ì—ˆë‹¤ëŠ” ëœ»ìœ¼ë¡œ ë§¨ ì•ì— 1ì„ ì¶”ê°€.\n",
    "alphas_bar_prev = F.pad(alphas_bar[:-1], (1, 0), value = 1)\n",
    "\n",
    "#trainingì— í•„ìš”í•œ ë³€ìˆ˜\n",
    "sqrt_alphas_bar = torch.sqrt(alphas_bar).to(device)\n",
    "sqrt_one_minus_alphas_bar = torch.sqrt(1. - alphas_bar).to(device)\n",
    "\n",
    "# samplingì— í•„ìš”í•œ ë³€ìˆ˜\n",
    "reciprocal_alphas_sqrt = torch.sqrt(1. / alphas_bar).to(device)\n",
    "reciprocal_alphasm1_sqrt = torch.sqrt(1. / alphas_bar - 1.).to(device)\n",
    "posterior_mean_coef1 = torch.sqrt(alphas_bar_prev) * betas / (1. - alphas_bar).to(device)\n",
    "posterior_mean_coef2 = torch.sqrt(alphas) * (1. - alphas_bar_prev) / (1. - alphas_bar).to(device)\n",
    "sigmas = (betas * (1. - alphas_bar_prev) / (1. - alphas_bar)).to(device) # ì‹œê·¸ë§ˆëŠ” betasë¥¼ ì‚¬ìš©í•´ë„ ë¬´ê´€í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba39202-e5da-4337-a0c2-f608b3a9781f",
   "metadata": {},
   "source": [
    "### í›ˆë ¨ ì½”ë“œ\n",
    "\n",
    "1. **repeat**:\n",
    "   - \\($ \\mathbf{x}_0 \\sim q(\\mathbf{x}_0) $\\)  (ë°ì´í„° ë¶„í¬ì—ì„œ ìƒ˜í”Œë§)\n",
    "   - \\($ t \\sim \\text{Uniform}(\\{1, \\dots, T\\})$ \\)  (ëœë¤í•œ ì‹œê°„ ìŠ¤í… ìƒ˜í”Œë§)\n",
    "   - \\($ \\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ \\)  (ì •ê·œë¶„í¬ì—ì„œ ë…¸ì´ì¦ˆ ìƒ˜í”Œë§)\n",
    "   - Gradient descent step on:\n",
    "\n",
    "   - $\n",
    "     \\nabla_\\theta \\left\\| \\epsilon - \\epsilon_\\theta \\left( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t \\right) \\right\\|^2\n",
    "     $\n",
    "\n",
    "2. **until converged**\n",
    "\n",
    "<details>\n",
    "  <summary> torch ì˜ˆì œ ì½”ë“œ (gather, view í•¨ìˆ˜) </summary>\n",
    "\n",
    "1. gather í•¨ìˆ˜\n",
    "    \n",
    "    ```python\n",
    "        # ì›ë³¸ í…ì„œ\n",
    "        coeff = torch.tensor([10, 20, 30, 40, 50])\n",
    "        \n",
    "        # íŠ¹ì • ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ t í…ì„œ\n",
    "        t = torch.tensor([0, 2, 4])  # ì¸ë±ìŠ¤ 0, 2, 4ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì§€ì •\n",
    "        \n",
    "        # gather ì‚¬ìš© (dim=0)\n",
    "        coeff_t = torch.gather(coeff, index=t, dim=0)\n",
    "        # ì¶œë ¥: tensor([10, 30, 50])\n",
    "    ```\n",
    "    \n",
    "2. view í•¨ìˆ˜\n",
    "\n",
    "    ```python\n",
    "    # ì˜ˆì œ í…ì„œ\n",
    "    coeff_t = torch.tensor([1, 2, 3, 4])  # Shape: (4,)\n",
    "    \n",
    "    # ë³€í™˜í•  ì°¨ì› ë¦¬ìŠ¤íŠ¸\n",
    "    dims = [2, 2]  # ì—¬ê¸°ì„œ len(dims) = 2\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "    B = coeff_t.shape[0] # 4\n",
    "    reshaped_tensor = coeff_t.view([B] + [1] * len(dims))\n",
    "    # ì¶œë ¥: torch.Size([4, 1, 1])\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7c11c7-999c-45fe-ab5b-6d2655d322ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_and_expand(coeff, t, xshape):\n",
    "    ''' \n",
    "    tì‹œê°„ì— í•´ë‹¹í•˜ëŠ” ê³„ìˆ˜(ì¸ë±ìŠ¤)ë¥¼ ê³„ìˆ˜ í…ì„œ(coeff)ì—ì„œ ê°€ì ¸ì˜¤ê³ , \n",
    "    í•´ë‹¹ ê³„ìˆ˜ë“¤ì„ batch sizeì— ë§ê²Œ í™•ì¥í•˜ëŠ” í•¨ìˆ˜ \n",
    "    '''\n",
    "    # ì…ë ¥ í…ì„œì˜ ì°¨ì› ë¶„ë¦¬\n",
    "    batch_size, *dims = xshape # batch_sizeëŠ” ì²« ë²ˆì§¸ ì°¨ì›, dimsëŠ” ë‚˜ë¨¸ì§€ ì°¨ì›\n",
    "\n",
    "     # tì‹œê°„ì— í•´ë‹¹í•˜ëŠ” ê³„ìˆ˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    coeff_t = torch.gather(coeff, index = t, dim = 0) # Shape: (len(t), ) = (batch_size, )\n",
    "    \n",
    "    # ì°¨ì› í™•ì¥\n",
    "    coeff_t = coeff_t.view([batch_size] + [1] * len(dims)) # ë‚˜ë¨¸ì§€ ì°¨ì›ë“¤ì— ë§ê²Œ í™•ì¥ -> ì´í›„ì˜ ê³„ì‚° ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•˜ì—¬\n",
    "    return coeff_t\n",
    "    \n",
    "def train(model, x_0): # x_0: ë°ì´í„° ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ì…ë ¥ ë°ì´í„°\n",
    "    # ëœë¤í•œ ì‹œê°„ ìŠ¤í…ì—ì„œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ(x_0.shape[0])ë§Œí¼ ìƒ˜í”Œë§\n",
    "    t = torch.randint(T, size = (x_0.shape[0], ), device = x_0.device)\\\n",
    "    \n",
    "    # ì •ê·œë¶„í¬ì—ì„œ ë…¸ì´ì¦ˆ ìƒ˜í”Œë§, shapeì´ batchì™€ ê°™ê²Œ\n",
    "    eps = torch.randn_like(x_0)\n",
    "    \n",
    "    # model input, batch ë“¤ê°„ ê³„ìˆ˜\n",
    "    # ëª¨ë“  batchê°€ ê°™ì€ të¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼í•¨\n",
    "    # torch.Size([32, 1, 1, 1])  32: batch_size\n",
    "    x_t = gather_and_expand(sqrt_alphas_bar, t, x_0.shape)*x_0 + gather_and_expand(sqrt_one_minus_alphas_bar, t, x_0.shape) * eps\n",
    "\n",
    "    # epsì™€ model output mse êµ¬í•˜ê¸°\n",
    "    loss = F.mse_loss(model(x_t, t), eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46c59a-e665-478b-b8e1-ddf0801cc9d4",
   "metadata": {},
   "source": [
    "### ìƒ˜í”Œë§ ì½”ë“œ\n",
    "1. **Initialize**:  \n",
    "       - \\($ \\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) $\\)  (ì‹œì‘ ìƒ˜í”Œì€ í‘œì¤€ ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§)\n",
    "    \n",
    "2. **For \\( t = T, ..., 1 \\) do**:\n",
    "   - \\($ \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ \\) if \\($ t > 1 $\\), else \\($ \\mathbf{z} = 0 $\\)  \n",
    "     (ë§ˆì§€ë§‰ ë‹¨ê³„ê°€ ì•„ë‹ˆë©´ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€)\n",
    "\n",
    "   - ì—…ë°ì´íŠ¸:\n",
    "     $\n",
    "     \\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \n",
    "     \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(\\mathbf{x}_t, t) \\right) \n",
    "     + \\sigma_t \\mathbf{z}\n",
    "     $\n",
    "       - êµ¬í˜„ ì‹œì—” $\\mu_\\theta(x_t,t) = \\tilde{\\mu}_t \\left( \\mathbf{x}_t, \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( \\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta} (\\mathbf{x}_t) \\right) \\right)$ ì˜ ìˆ˜ì‹ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•œë‹¤.\n",
    "       - ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì•„ì„œ $ \\tilde{\\mu}_t (\\mathbf{x}_t, \\mathbf{x}_0) := \n",
    "\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0 \n",
    "+ \\frac{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t$ , $x_0$ì€ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤ $x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \n",
    "\\left( \\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta} (\\mathbf{x}_t) \\right)$\n",
    "\n",
    "3. **Return** \\( $\\mathbf{x}_0 $\\)  (ìµœì¢… ìƒì„±ëœ ìƒ˜í”Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e56118c-d904-4648-a82c-31665d48613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ê¸°ì¡´\n",
    "def sample(model, x_T): # x_T: noisy data\n",
    "    x_t = x_T\n",
    "    for time_step in reversed(range(T)): # T , ..., 1 ë°˜ë³µ ìˆ˜í–‰\n",
    "        # ê° time_stepë¥¼ batch sizeë¡œ í™•ì¥\n",
    "        t = torch.full((x_T.shape[0], ), time_step, dtype=torch.long, device=device)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ë‹¨ê³„ê°€ ì•„ë‹ˆë©´ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒ˜í”Œë§\n",
    "        z = torch.randn_like(x_t) if time_step else 0\n",
    "\n",
    "        # ì—…ë°ì´íŠ¸ ê³¼ì •\n",
    "        eps = model(x_t, t) # ëª¨ë¸ ì˜ˆì¸¡\n",
    "\n",
    "        # x_0ì„ êµ¬í•¨\n",
    "        # ì•„ë˜ì™€  ê°™ì´ sampling ì‹œì—” batchë§ˆë‹¤ tê°’ì´ ê°ê° ê°™ê¸° ë•Œë¬¸ì— gather í•¨ìˆ˜ë¥¼ ì•ˆì¨ë„ ëœë‹¤.\n",
    "        # print(f\"ê¸°ì¡´ ì°¨ì›: {gather_and_expand(reciprocal_alphas_sqrt, t, eps.shape).shape} \\n  \\\n",
    "        #         view ì°¨ì›: {reciprocal_alphas_sqrt[t].view( [len(t)]+ [1] * len(eps.shape[1:]) ).shape}\")\n",
    "              \n",
    "        x0_predicted = gather_and_expand(reciprocal_alphas_sqrt, t, eps.shape) * x_t - \\\n",
    "            gather_and_expand(reciprocal_alphasm1_sqrt, t, eps.shape) * eps\n",
    "        \n",
    "        # ìœ„ x_0ê³¼ í•¨ê»˜ í‰ê· ì„ êµ¬í•œë‹¤\n",
    "        mean = gather_and_expand(posterior_mean_coef1, t, eps.shape) * x0_predicted + \\\n",
    "            gather_and_expand(posterior_mean_coef2, t, eps.shape) * x_t\n",
    "        \n",
    "        # ë¶„ì‚° êµ¬í•¨\n",
    "        var = torch.sqrt(gather_and_expand(sigmas, t, eps.shape)) * z\n",
    "\n",
    "        x_t = mean + var\n",
    "        \n",
    "    # ë§ˆì§€ë§‰ ê²°ê³¼ return \n",
    "    x_0 = x_t\n",
    "    return x_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac441c0-08fd-496e-9421-d17aa78c8caa",
   "metadata": {
    "id": "bO-ZCNef7TvQ"
   },
   "source": [
    "## **Prepare Dataset/Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f9097d-71be-4c2a-8a23-bbb4f535e191",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "11af90112c6e4d5f9d17265ea1be76e8",
      "ca43c3ef84a14d8b958c9f9e352ed68c",
      "fceb31b8d8954035a5cd2611fa1db129",
      "4917f4a7bcb4479885fc07d26d48714a",
      "0360c81f520e458d8387bb6f73ef4d34",
      "1337223e98864f4e95e3a3e31d26b11e",
      "936ef8ef40634ce7b90a92c5da8ba076",
      "bd854a978db84b00b8bedf40cd2536a2",
      "cc4eed40fc194805a5c09ca4f736382b",
      "53cb0d32f0d24aceb5e470a584e07807",
      "95a87db3914a4b4eb823242c03a5ebb8"
     ]
    },
    "id": "P6uQCHvLoPBM",
    "outputId": "20e261a7-4c44-41a4-f119-6dc8aaa668ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10(\n",
    "    root=\"./data\", train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=32, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3aabf-e027-45cd-9b2a-85ccbf730f56",
   "metadata": {
    "id": "LlXogTCfsoEx"
   },
   "source": [
    "## **Model architecture**\n",
    "\n",
    "https://github.com/w86763777/pytorch-ddpm/blob/master/model.py\n",
    "\n",
    "ìœ„ githubì—ì„œ copyí•¨(\\_\\_name\\_\\_ == \"\\_\\_name\\_\\_\" ì œì™¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9458bddc-02a2-47fc-96f6-16650f1c20b1",
   "metadata": {
    "id": "0OBLj-cbqnsm"
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, T, d_model, dim):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
    "        emb = torch.exp(-emb)\n",
    "        pos = torch.arange(T).float()\n",
    "        emb = pos[:, None] * emb[None, :]\n",
    "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        emb = emb.view(T, d_model)\n",
    "\n",
    "        self.timembedding = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(d_model, dim),\n",
    "            Swish(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        emb = self.timembedding(t)\n",
    "        return emb\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        _, _, H, W = x.shape\n",
    "        x = F.interpolate(\n",
    "            x, scale_factor=2, mode='nearest')\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
    "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in [self.proj_q, self.proj_k, self.proj_v, self.proj]:\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.proj.weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        q = self.proj_q(h)\n",
    "        k = self.proj_k(h)\n",
    "        v = self.proj_v(h)\n",
    "\n",
    "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        k = k.view(B, C, H * W)\n",
    "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
    "        assert list(w.shape) == [B, H * W, H * W]\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        h = torch.bmm(w, v)\n",
    "        assert list(h.shape) == [B, H * W, C]\n",
    "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        h = self.proj(h)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=False):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.temb_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(32, out_ch),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        if attn:\n",
    "            self.attn = AttnBlock(out_ch)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.block2[-1].weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.block1(x)\n",
    "        h += self.temb_proj(temb)[:, :, None, None]\n",
    "        h = self.block2(h)\n",
    "\n",
    "        h = h + self.shortcut(x)\n",
    "        h = self.attn(h)\n",
    "        return h\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, T, ch, ch_mult, attn, num_res_blocks, dropout):\n",
    "        super().__init__()\n",
    "        assert all([i < len(ch_mult) for i in attn]), 'attn index out of bound'\n",
    "        tdim = ch * 4\n",
    "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
    "\n",
    "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        chs = [ch]  # record output channel when dowmsample for upsample\n",
    "        now_ch = ch\n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.downblocks.append(ResBlock(\n",
    "                    in_ch=now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=(i in attn)))\n",
    "                now_ch = out_ch\n",
    "                chs.append(now_ch)\n",
    "            if i != len(ch_mult) - 1:\n",
    "                self.downblocks.append(DownSample(now_ch))\n",
    "                chs.append(now_ch)\n",
    "\n",
    "        self.middleblocks = nn.ModuleList([\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
    "        ])\n",
    "\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.upblocks.append(ResBlock(\n",
    "                    in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=(i in attn)))\n",
    "                now_ch = out_ch\n",
    "            if i != 0:\n",
    "                self.upblocks.append(UpSample(now_ch))\n",
    "        assert len(chs) == 0\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.GroupNorm(32, now_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.head.weight)\n",
    "        init.zeros_(self.head.bias)\n",
    "        init.xavier_uniform_(self.tail[-1].weight, gain=1e-5)\n",
    "        init.zeros_(self.tail[-1].bias)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Timestep embedding\n",
    "        temb = self.time_embedding(t)\n",
    "        # Downsampling\n",
    "        h = self.head(x)\n",
    "        hs = [h]\n",
    "        for layer in self.downblocks:\n",
    "            h = layer(h, temb)\n",
    "            hs.append(h)\n",
    "        # Middle\n",
    "        for layer in self.middleblocks:\n",
    "            h = layer(h, temb)\n",
    "        # Upsampling\n",
    "        for layer in self.upblocks:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = layer(h, temb)\n",
    "        h = self.tail(h)\n",
    "\n",
    "        assert len(hs) == 0\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d552057-9a89-49cc-a003-05cab7d0edd0",
   "metadata": {
    "id": "Xu99fy1220Ej"
   },
   "source": [
    "## **Make model,optimizer, scheduler instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3da4698-cbb8-4c94-99cd-380f111025a9",
   "metadata": {
    "id": "FWuQImErt6Rp"
   },
   "outputs": [],
   "source": [
    "model = UNet(T=T, ch=128, ch_mult=[1, 2, 2, 1], attn=[1],\n",
    "             num_res_blocks=2, dropout=0.1).to(device)\n",
    "#ema_model = copy.deepcopy(model)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "#sched = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=warmup_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8e392-854d-4420-9ef3-b680274bc671",
   "metadata": {
    "id": "mbgrZo237Gm-"
   },
   "source": [
    "## **Train Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada15dc-5727-4fcf-8ab3-af081110fd55",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nnX_9l45uNIE",
    "outputId": "ad7b5a50-1491-4065-b43f-f682b7c95d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1 , Iter: 939/1563]  Loss: 0.026"
     ]
    }
   ],
   "source": [
    "# %%capture output\n",
    "for e in range(1, 20+1):\n",
    "    model.train()\n",
    "    for i, (x, _) in enumerate(dataloader, 1):\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        loss = train(model, x)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(\"\\r[Epoch: {} , Iter: {}/{}]  Loss: {:.3f}\".format(e, i, len(dataloader), loss.item()), end='')\n",
    "    print(\"\\n> Eval at epoch {}\".format(e))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_T = torch.randn(5, 3, 32, 32).to(device)\n",
    "        x_0 = sample(model, x_T)\n",
    "        x_0 = x_0.permute(0, 2, 3, 1).clamp(0, 1).detach().cpu().numpy() * 255\n",
    "        for i in range(5):\n",
    "            # plt.imshow(cv2.cvtColor(x_0[i].astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "            plt.figure(figsize=(1.5, 1.5))  # ğŸ”¹ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ (2x2 ì¸ì¹˜)\n",
    "            plt.imshow(x_0[i].astype('uint8'))\n",
    "            plt.axis(\"off\")  # ì¶• ì œê±°\n",
    "            plt.show()\n",
    "# # ì¶œë ¥ ë‚´ìš©ì„ íŒŒì¼ë¡œ ì €ì¥\n",
    "# with open(\"output.log\", \"w\") as f:\n",
    "#     f.write(output.stdout)  \n",
    "# # ì €ì¥ëœ ë‚´ìš© ì¶œë ¥\n",
    "# with open(\"output.log\", \"r\") as f:\n",
    "#     print(f.read())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e134cb2-1e5f-40df-a927-24302897a6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
